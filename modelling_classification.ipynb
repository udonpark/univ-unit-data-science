{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT1043 Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <center> Assignment 2 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yo Kogure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32134541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a report summarizing my attempt on the Assignment 2 of FIT1043.\n",
    "I will try to keep the report as detailed, clean, appropriately formatted and easy to read as possible.\n",
    "\n",
    "When I am not citing a reference, that means that I am not copy-pasting or referring to other site. However I do refer to lecture notes and totorial exercises without stating it each time.\n",
    "\n",
    "References are stated with APA format at the conclusion at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file mostly consists as follows:\n",
    "1. Introduction\n",
    "2. Supervised Learning\n",
    "3. Classification\n",
    "4. Kaggle attempts\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will now import necessary libraries, and read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # numpy helps us deal with numbers in the future\n",
    "import pandas as pd  # pandas, as we talked in the assignment 1, it allows us deal with data and wrangle.\n",
    "import matplotlib.pyplot as plt  # this library allows us to plot many different kind of graphs and visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on as we need it, I will download some more libraries such as sklearn.model ( which I will be covering later). I only actually needed pandas here to load csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_data = pd.read_csv(\"FIT1043-Essay-Features.csv\")  # loading csv file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 1332 rows and 19 columns each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>469</td>\n",
       "      <td>3049</td>\n",
       "      <td>648</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4.705247</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>15.069767</td>\n",
       "      <td>643.659420</td>\n",
       "      <td>0.993302</td>\n",
       "      <td>227</td>\n",
       "      <td>0.350309</td>\n",
       "      <td>123</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>738</td>\n",
       "      <td>725</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>431</td>\n",
       "      <td>3554</td>\n",
       "      <td>694</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.121037</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>33.047619</td>\n",
       "      <td>691.325626</td>\n",
       "      <td>0.996146</td>\n",
       "      <td>306</td>\n",
       "      <td>0.440922</td>\n",
       "      <td>160</td>\n",
       "      <td>0.230548</td>\n",
       "      <td>744</td>\n",
       "      <td>719</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>79</td>\n",
       "      <td>650</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.887218</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>131.984733</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>59</td>\n",
       "      <td>0.443609</td>\n",
       "      <td>34</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1640</td>\n",
       "      <td>1468</td>\n",
       "      <td>300</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.893333</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>299.328859</td>\n",
       "      <td>0.997763</td>\n",
       "      <td>136</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>70</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>371</td>\n",
       "      <td>357</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1119</td>\n",
       "      <td>1774</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>4.730667</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>373.329749</td>\n",
       "      <td>0.995546</td>\n",
       "      <td>154</td>\n",
       "      <td>0.410667</td>\n",
       "      <td>104</td>\n",
       "      <td>0.277333</td>\n",
       "      <td>412</td>\n",
       "      <td>405</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>914</td>\n",
       "      <td>1979</td>\n",
       "      <td>390</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5.074359</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>389.329897</td>\n",
       "      <td>0.998282</td>\n",
       "      <td>172</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>99</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>466</td>\n",
       "      <td>445</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>485</td>\n",
       "      <td>1095</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.367647</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>15.692308</td>\n",
       "      <td>198.979899</td>\n",
       "      <td>0.975392</td>\n",
       "      <td>88</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>58</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>280</td>\n",
       "      <td>275</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>529</td>\n",
       "      <td>1982</td>\n",
       "      <td>393</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.043257</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>389.984655</td>\n",
       "      <td>0.992327</td>\n",
       "      <td>187</td>\n",
       "      <td>0.475827</td>\n",
       "      <td>98</td>\n",
       "      <td>0.249364</td>\n",
       "      <td>498</td>\n",
       "      <td>491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>737</td>\n",
       "      <td>1598</td>\n",
       "      <td>341</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.686217</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>337.994065</td>\n",
       "      <td>0.991185</td>\n",
       "      <td>157</td>\n",
       "      <td>0.460411</td>\n",
       "      <td>86</td>\n",
       "      <td>0.252199</td>\n",
       "      <td>404</td>\n",
       "      <td>395</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1391</td>\n",
       "      <td>952</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.621359</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13.733333</td>\n",
       "      <td>203.990148</td>\n",
       "      <td>0.990243</td>\n",
       "      <td>76</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>69</td>\n",
       "      <td>0.334951</td>\n",
       "      <td>243</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "677       469   3049    648      26           19             2   \n",
       "442       431   3554    694       7            4             4   \n",
       "121        79    650    133       3            5             0   \n",
       "970      1640   1468    300       8            6             0   \n",
       "300      1119   1774    375       4           19             1   \n",
       "557       914   1979    390      13           14             0   \n",
       "966       485   1095    204       5            6             0   \n",
       "1064      529   1982    393       3            8             0   \n",
       "1263      737   1598    341      27            3             0   \n",
       "650      1391    952    206       0            2             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "677          4.705247         43          2          15.069767  643.659420   \n",
       "442          5.121037         21          1          33.047619  691.325626   \n",
       "121          4.887218          7          0          19.000000  131.984733   \n",
       "970          4.893333         16          0          18.750000  299.328859   \n",
       "300          4.730667         18          1          20.833333  373.329749   \n",
       "557          5.074359         20          2          19.500000  389.329897   \n",
       "966          5.367647         13          0          15.692308  198.979899   \n",
       "1064         5.043257         24          3          16.375000  389.984655   \n",
       "1263         4.686217          5          0          68.200000  337.994065   \n",
       "650          4.621359         15          0          13.733333  203.990148   \n",
       "\n",
       "      POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "677          0.993302           227                  0.350309            123   \n",
       "442          0.996146           306                  0.440922            160   \n",
       "121          0.992366            59                  0.443609             34   \n",
       "970          0.997763           136                  0.453333             70   \n",
       "300          0.995546           154                  0.410667            104   \n",
       "557          0.998282           172                  0.441026             99   \n",
       "966          0.975392            88                  0.431373             58   \n",
       "1064         0.992327           187                  0.475827             98   \n",
       "1263         0.991185           157                  0.460411             86   \n",
       "650          0.990243            76                  0.368932             69   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \n",
       "677                    0.189815        738      725      4  \n",
       "442                    0.230548        744      719      3  \n",
       "121                    0.255639        173      173      3  \n",
       "970                    0.233333        371      357      3  \n",
       "300                    0.277333        412      405      2  \n",
       "557                    0.253846        466      445      3  \n",
       "966                    0.284314        280      275      3  \n",
       "1064                   0.249364        498      491      4  \n",
       "1263                   0.252199        404      395      4  \n",
       "650                    0.334951        243      238      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have randomly taken a sample of 5 rows.\n",
    "To roughly give some description of the data I have read, I will not repeat what each of these columns indicate, because it is already done so in the specification sheet.\n",
    "\n",
    "#### This sets of data contains information on 1332 different essays. It stores details of each essay regarding words and characters, such as total characters, average of each word length, number of synonyms, etc. There are 19 different columns so 19 different characteristics/features stored for this set of data.\n",
    "\n",
    "#### Ultimately, I believe that our goal is to build a model to predict its \"score\" from 1-6, from sets of given features of each essay.\n",
    "One that approximates y = f(x), where y is the score and x are all other features.\n",
    "\n",
    "I will assume that data is already clean and wrangled to some point, because so far from what I have checked manually, they are, with no NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised machine learning means, a part of machine learning feature that includes analyzing the data to find out a suitable function that maps a set of inputs to its corresponding outputs. For a set of x and y, its aim is to find a certain function that mostly accurately predicts value of y given a certain value of x.\n",
    "\n",
    "One feature of \"supervised learning\" is that the algorithm uses labeled data, to improve its accuracy on the prediction of given data. It also efficiently use train datasets to analyze and test datasets to actually test the algorithm you have found out. \n",
    "\n",
    "What does labeled data mean? For example for the above data, its \"raw\" data contains thousands of datasets for each 19 different columns. They are just \"unlabeled\" numbers and even machines can not process until we label them and make it valuable for the machine learning. It extracts necessary fields, and adds meaningful label or tag to the raw data in order to perform supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply supervised learning, datasets must be set to training datasets and testing datasets.\n",
    "Training datasets are data that the algorithm takes in as an input to learn and find the function. Based on train data, an appropriate model is implemented. \n",
    "Test datasets are those that are used to check how accurate the function or model is, after algorithm finds out suitable model based on training datasets.\n",
    "\n",
    "For example out of 100 sets of given data, if 100 are used for training, and use same 100 for testing, it doesn't allow us to test from neutral standpoint. Because that model is built by analyzing all 100 data in the first place, of course the test will return positive result. This won't be a fair accuracy test unless we divide them into training datasets and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, features basically means property of your training data. It is like x values from which we will take in as input.\n",
    "\n",
    "Label is like y values, in short. It is a set of output that we get after training with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the label here is 'score', because we want to find and predict it as an output from 1-6 given set of other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will seperate features and label using pd.iloc[] ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• essayid - a unique id to identify the essay <br>\n",
    "• chars - number of characters in the essay, including spaces <br>\n",
    "• words - number of words in the essay <br>\n",
    "• commas - number of commas in the essay <br>\n",
    "• apostrophes - number of apostrophes in the essay <br>\n",
    "• punctuations - number of punctuations (other than commas, apostrophes, period,\n",
    "questions marks in the essay <br>\n",
    "• avg_word_length - the average length of the words in the essay <br>\n",
    "• sentences - number of sentences in the essay, determined by the period (fullstops) <br>\n",
    "• questions - number of questions in the essay, determined by the question marks <br>\n",
    "• avg_word_sentence - the average number of words in a sentence in the essay <br>\n",
    "• POS - total number of Part-of-Speech discovered <br>\n",
    "• POS/total_words - fraction of the POS in the total number of words in the essay <br>\n",
    "• prompt_words - words that are related to the essay topic <br>\n",
    "• prompt_words/total_words - fraction of the prompt words in the total number of\n",
    "words in the essay <br>\n",
    "• synonym_words - words that are synonymous <br>\n",
    "• synonym_words/total_words - fraction of the synonymous words in the total\n",
    "number of words in the essay <br>\n",
    "• unstemmed - number of words that were not stemmed in the essay <br>\n",
    "• stemmed - number of words that were stemmed (cut to the based word) in the essay <br>\n",
    "• score - the rating grade, ranging from 1 – 6 <br>\n",
    "### Copied and pasted from specification sheet for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### I am allowed to engineer or remove features as I deem appropriate,  so will be doing so at earlier stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be excluding essayid from features because it does not affect grade, they are just referencing numbers. For now only essayid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del essay_data[\"essayid\"]  # remove column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0   2153    426      14            6             0         5.053991   \n",
       "1   1480    292       9            7             0         5.068493   \n",
       "2   3964    849      19           26             1         4.669022   \n",
       "3    988    210       8            7             0         4.704762   \n",
       "4   3139    600      13            8             0         5.231667   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         16          0          26.625000  423.995272         0.995294   \n",
       "1         11          0          26.545455  290.993103         0.996552   \n",
       "2         49          2          17.326531  843.990544         0.994100   \n",
       "3         12          0          17.500000  207.653784         0.988828   \n",
       "4         24          1          25.000000  594.652150         0.991087   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           207                  0.485915            105   \n",
       "1           148                  0.506849             77   \n",
       "2           285                  0.335689            130   \n",
       "3           112                  0.533333             62   \n",
       "4           255                  0.425000            165   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                   0.246479        424      412      4  \n",
       "1                   0.263699        356      345      4  \n",
       "2                   0.153121        750      750      4  \n",
       "3                   0.295238        217      209      3  \n",
       "4                   0.275000        702      677      4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = essay_data.iloc[:, -1].values\n",
    "y  # y is the label, because it acts as an output, for \"score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.15300000e+03, 4.26000000e+02, 1.40000000e+01, ...,\n",
       "        2.46478873e-01, 4.24000000e+02, 4.12000000e+02],\n",
       "       [1.48000000e+03, 2.92000000e+02, 9.00000000e+00, ...,\n",
       "        2.63698630e-01, 3.56000000e+02, 3.45000000e+02],\n",
       "       [3.96400000e+03, 8.49000000e+02, 1.90000000e+01, ...,\n",
       "        1.53121319e-01, 7.50000000e+02, 7.50000000e+02],\n",
       "       ...,\n",
       "       [1.81400000e+03, 3.63000000e+02, 5.00000000e+00, ...,\n",
       "        2.94765840e-01, 4.27000000e+02, 4.15000000e+02],\n",
       "       [1.42700000e+03, 2.87000000e+02, 5.00000000e+00, ...,\n",
       "        2.89198606e-01, 3.23000000e+02, 3.12000000e+02],\n",
       "       [2.80600000e+03, 5.42000000e+02, 2.40000000e+01, ...,\n",
       "        2.85977860e-01, 5.96000000e+02, 5.75000000e+02]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = essay_data.iloc[:, :-1].values  # X contains all other features excluding score\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will import necessary library to split my data for training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)  \n",
    "# test_size = 0.25 means that 75% is used for training and 25% is used for testing\n",
    "# random_state affects how to decide how they are seperated to training and testing. \n",
    "# Because it is =0, for this case they are seperated very randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 333, 999, 333)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) , len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There! They are correctly seperated into training and testing datasets. The ratio can honestly be anything, 80:20, 70:30 or 90:10 depending on the situaition, but I would like to take 75:25 as a balanced middle value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.34500000e+03, 4.90000000e+02, 9.00000000e+00, 1.10000000e+01,\n",
       "         0.00000000e+00, 4.78571429e+00, 1.70000000e+01, 0.00000000e+00,\n",
       "         2.88235294e+01, 4.86991786e+02, 9.93860789e-01, 2.04000000e+02,\n",
       "         4.16326531e-01, 1.27000000e+02, 2.59183673e-01, 5.07000000e+02,\n",
       "         4.95000000e+02],\n",
       "        [3.01000000e+03, 6.21000000e+02, 4.80000000e+01, 7.00000000e+00,\n",
       "         0.00000000e+00, 4.84702093e+00, 2.80000000e+01, 1.00000000e+00,\n",
       "         2.21785714e+01, 6.16987076e+02, 9.93537964e-01, 3.15000000e+02,\n",
       "         5.07246377e-01, 1.44000000e+02, 2.31884058e-01, 6.72000000e+02,\n",
       "         6.61000000e+02],\n",
       "        [2.09900000e+03, 4.20000000e+02, 1.10000000e+01, 5.00000000e+00,\n",
       "         0.00000000e+00, 4.99761905e+00, 2.00000000e+01, 0.00000000e+00,\n",
       "         2.10000000e+01, 4.15320513e+02, 9.88858364e-01, 2.52000000e+02,\n",
       "         6.00000000e-01, 1.25000000e+02, 2.97619048e-01, 4.47000000e+02,\n",
       "         4.42000000e+02]]),\n",
       " array([[1.87000000e+03, 3.71000000e+02, 2.10000000e+01, 3.00000000e+00,\n",
       "         0.00000000e+00, 5.04043127e+00, 2.20000000e+01, 0.00000000e+00,\n",
       "         1.68636364e+01, 3.67324251e+02, 9.90092320e-01, 1.92000000e+02,\n",
       "         5.17520216e-01, 9.40000000e+01, 2.53369272e-01, 4.04000000e+02,\n",
       "         3.89000000e+02],\n",
       "        [1.53700000e+03, 2.94000000e+02, 1.50000000e+01, 8.00000000e+00,\n",
       "         0.00000000e+00, 5.22789116e+00, 1.40000000e+01, 2.00000000e+00,\n",
       "         2.10000000e+01, 2.92993151e+02, 9.96575342e-01, 1.24000000e+02,\n",
       "         4.21768707e-01, 6.20000000e+01, 2.10884354e-01, 3.80000000e+02,\n",
       "         3.72000000e+02],\n",
       "        [1.17900000e+03, 2.31000000e+02, 8.00000000e+00, 2.00000000e+00,\n",
       "         0.00000000e+00, 5.10389610e+00, 8.00000000e+00, 1.00000000e+00,\n",
       "         2.88750000e+01, 2.28327460e+02, 9.88430561e-01, 9.90000000e+01,\n",
       "         4.28571429e-01, 6.90000000e+01, 2.98701299e-01, 2.89000000e+02,\n",
       "         2.83000000e+02]]),\n",
       " array([4, 3, 3]),\n",
       " array([4, 4, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3], X_test[:3], y_train[:3], y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like it succeeded. Moving on to the next part of the assignmnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification is when the given dataset is categorized into 2 distinct classes in the supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, as its name suggests, they are not categorized into 2, but more than 2, multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept is fairly simple, but to give an example, if we try to implement a model to predict whether data showing whether students failed FIT1043 from given data, it is considered as binary classification, because only 2 distinct choices in the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a model to predict which grade that student took, is considered as multi-class classification, because there are multiple options to classify them like HD, D, N and F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of question requires knowledge of normalising of data. From that fact, I can predict that later on I will be using Support Vector Machine/Regression to deal with several multiple features more than 1(which I kind of already know). And I can also tell that some values of the input are either too large or too small. I will explain why I thought so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for normalising the data, is because some values for columns are just too large or too small. After normalisation, different values that were measured under different scales are set to a common scale, with 0 being the value in the common scale for the average value. Many classifiers that are in python libraries calculate the distance between two points by using Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>commas</th>\n",
       "      <th>questions</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>2569</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2221</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2108</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     chars  commas  questions  score\n",
       "734   2569      33          0      5\n",
       "119   2221      11          1      4\n",
       "280   2108      14          0      4\n",
       "225   2020       5          0      4\n",
       "705    173       0          0      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example,\n",
    "small_test = essay_data.filter(items=['chars', 'commas', 'questions','score'])\n",
    "small_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are unaware of weighting of importance of each features on the score. However, if we were to execute classification by finding distance between two points, (imagine drawing lines from each point to point and using pythagoras theorem to find its distance)values in chars will be too dominant. We want to consider commas, questions and of course other features fairly. They can't be too large or too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such case features do not contribute equally to our output, hence we have a need to re-scale them by using normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import necessary library for standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will apply scaling and transform its data in training datasets only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()  # StandardScaler() from sklearn.preprocessing is the function that I will be using\n",
    "X_train = sc.fit_transform(X_train)  # Train cases standardised, and transformed\n",
    "X_test = sc.transform(X_test)  \n",
    "# The mean and standard deviation value of train case is stored in sc, and applied to test cases now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.28684034,  0.38716127, -0.52496032,  0.44571172, -0.41870753,\n",
       "         -0.70522038, -0.23728237, -0.65162306,  0.40195483,  0.39407748,\n",
       "          0.52980282,  0.07996837, -1.03601228,  0.38954264, -0.12344141,\n",
       "          0.24983146,  0.26442177],\n",
       "        [ 1.03456971,  1.13092488,  2.98026314, -0.20693175, -0.41870753,\n",
       "         -0.4245677 ,  0.97890044, -0.12065125, -0.15102251,  1.13589011,\n",
       "          0.48749963,  1.40547103,  0.76146475,  0.76766351, -0.83510202,\n",
       "          1.2666706 ,  1.3115888 ]]),\n",
       " array([[-0.24725206, -0.28847132,  0.55356998, -0.85957522, -0.41870753,\n",
       "          0.46083592,  0.315528  , -0.65162306, -0.59331856, -0.28880026,\n",
       "          0.03598027, -0.06332921,  0.96457761, -0.34445669, -0.27501425,\n",
       "         -0.38492267, -0.40425116],\n",
       "        [-0.62167894, -0.72564535,  0.01430483, -0.04377088, -0.41870753,\n",
       "          1.3189993 , -0.56896859,  0.41032056, -0.24910037, -0.71296754,\n",
       "          0.88551956, -0.87534886, -0.92842096, -1.05621361, -1.38253326,\n",
       "         -0.53282655, -0.51149115]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2], X_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Support Vector Machine/Regression algorithm to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines, SVM are a set of supervised learning methods used for classification as well as regression algorithms. One of the benefits of SVM is that it can create the \"best decision boundary\" to separate two or more classes so that we can correctly place data points categorized into the correct class. \n",
    "\n",
    "One of its advantage is that it is effective is higher-dimentional spaces, that is, our features containing more than 10 different characteristics. Also I have learnt that it works relatively well in cases that there is a clear margin of seperation between classes. Since our label score are distinct integers ranging from 1-6, it also satisfies for our case.\n",
    "\n",
    "(The question asked in relation to Linear Regression): <br>\n",
    "Not only classification, but also for regression Support Vector Machine is useful. One of its advantage compared to usual Linear Regression is that SVM/SVR allows us to define for ourselves how much error is acceptable in our model and will find an appropriate line that fit the data.\n",
    "\n",
    "<br>\n",
    "*Reference:* <br>\n",
    "https://dataaspirant.com/svm-kernels/ <br>\n",
    "https://dhirajkumarblog.medium.com/top-4-advantages-and-disadvantages-of-support-vector-machine-or-svm-a3c06a2b107\n",
    "https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "There is something called kernel in Support Vector Machine/ Support Vector Regression.\n",
    "In my own words, you can consider kernel as a function in SVM that maps features to labels in supervised learning. Kernel machines are a class of algorithms for pattern analysis, and algorithms use a set of mathematical functions that are defined as the kernel.\n",
    "There are different kinds of kernels, and they include polynomial kernel, gaussian kernel, etc. <br>\n",
    "\n",
    "<br>\n",
    "*Reference:* <br>\n",
    "https://data-flair.training/blogs/svm-kernel-functions/ <br>\n",
    "https://en.wikipedia.org/wiki/Kernel_method <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have referred to: https://scikit-learn.org/stable/modules/svm.html for implementation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 333, 999, 333)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) , len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm  # import support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the built-in function: <br>\n",
    "- An array X of shape = (n_samples, n_classes), where n_samples is number of samples and n_classes is number of features. This is basically X_train that we have defined earlier.\n",
    "- An array y of class variable of shape (n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999, 17), (999,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()  # Create an object clf of class SVC, basically we will now start applying the algorithm\n",
    "clf.fit(X_train, y_train)  # It has been fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model has been successfully created and its information saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will now conduct a prediction for the 'score', using testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 4, 3, 4, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 3, 4,\n",
       "       3, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 2, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 4, 4,\n",
       "       4, 4, 4, 4, 3, 3, 4, 5, 4, 3, 3, 4, 4, 3, 4, 3, 4, 3, 3, 4, 4, 3,\n",
       "       4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 4, 2, 4, 4, 3, 4, 4, 3, 4, 4, 3, 4,\n",
       "       2, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 4, 4,\n",
       "       4, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 4, 3,\n",
       "       4, 4, 4, 2, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 4, 4,\n",
       "       3, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "       4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 4, 3, 2, 2, 3, 3, 3, 4, 3,\n",
       "       3, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 4, 4, 2, 3, 4, 3,\n",
       "       3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4,\n",
       "       4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3,\n",
       "       4, 4, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)  # apply the prediction on X_test dataset and name it as y_pred\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  # Import confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   2,   0,   0,   0,   0],\n",
       "       [  0,   8,  13,   2,   0,   0],\n",
       "       [  0,   1, 103,  43,   0,   0],\n",
       "       [  0,   0,  34, 109,   1,   0],\n",
       "       [  0,   0,   0,  16,   0,   0],\n",
       "       [  0,   0,   0,   1,   0,   0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our confusion matrix. It is a 6x6 matrix because our label, score has discrete choices of 6: 1, 2, 3, 4, 5, 6. If I were to explain some of the elements here, notice 103 at cm[2, 2]. This means that there are 103 data in testing dataset that we predicted is 3, and actually turned out to have a score of 3. 43 at cm[2, 3] shows that there are 43 test cases where we predicted them to have a score of 4, but actually had 3. You can look at diagnoals in the N*N matrix to see which one you have accurately predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Weighted Kappa(QWK) measures an agreement between two ratings. In our case two ratings mean our predicted scores and the actual/know scores. Agreement means that I have predicted a certain classification correctly, while disagreement means I have failed to do so. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In cases such that there is less agreement between the raters than expected, the metric may go below 0. \n",
    "\n",
    "\n",
    "<br>\n",
    "Reference:<br>\n",
    "https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find Quadratic Weighted Kappa, I will use cohen_kappa_score from sklearn.metrics library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5735877574154544"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn.metrics.cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
    "kappa_value = cohen_kappa_score(y_test, y_pred, labels=None, weights='quadratic', sample_weight=None) \n",
    "# weights = quadratic because this is quadratic weighter kappa\n",
    "kappa_value\n",
    "# Refered to: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have successfully obtained the QWK Score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will read the file and use the model I built earlier to predict the 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('FIT1043-Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4332</td>\n",
       "      <td>900</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.813333</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>893.988852</td>\n",
       "      <td>0.993321</td>\n",
       "      <td>392</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>196</td>\n",
       "      <td>0.217778</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>1465</td>\n",
       "      <td>280</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.232143</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>278.321343</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>131</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>51</td>\n",
       "      <td>0.182143</td>\n",
       "      <td>339</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>1696</td>\n",
       "      <td>325</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.218462</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17.105263</td>\n",
       "      <td>321.316770</td>\n",
       "      <td>0.988667</td>\n",
       "      <td>178</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>92</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>352</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>2640</td>\n",
       "      <td>555</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4.756757</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.821429</td>\n",
       "      <td>551.989150</td>\n",
       "      <td>0.994575</td>\n",
       "      <td>228</td>\n",
       "      <td>0.410811</td>\n",
       "      <td>107</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>632</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>2844</td>\n",
       "      <td>596</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.771812</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24.833333</td>\n",
       "      <td>593.658810</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>279</td>\n",
       "      <td>0.468121</td>\n",
       "      <td>138</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>626</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  chars  words  commas  apostrophes  punctuations  avg_word_length  \\\n",
       "0     1623   4332    900      28           13             0         4.813333   \n",
       "1     1143   1465    280      11            3             1         5.232143   \n",
       "2      660   1696    325      17            2             0         5.218462   \n",
       "3     1596   2640    555      20           17             0         4.756757   \n",
       "4      846   2844    596      33            4             1         4.771812   \n",
       "\n",
       "   sentences  questions  avg_word_sentence         POS  POS/total_words  \\\n",
       "0         39          1          23.076923  893.988852         0.993321   \n",
       "1         14          3          20.000000  278.321343         0.994005   \n",
       "2         19          1          17.105263  321.316770         0.988667   \n",
       "3         28          0          19.821429  551.989150         0.994575   \n",
       "4         24          9          24.833333  593.658810         0.996072   \n",
       "\n",
       "   prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0           392                  0.435556            196   \n",
       "1           131                  0.467857             51   \n",
       "2           178                  0.547692             92   \n",
       "3           228                  0.410811            107   \n",
       "4           279                  0.468121            138   \n",
       "\n",
       "   synonym_words/total_words  unstemmed  stemmed  \n",
       "0                   0.217778        750      750  \n",
       "1                   0.182143        339      316  \n",
       "2                   0.283077        352      337  \n",
       "3                   0.192793        632      605  \n",
       "4                   0.231544        626      607  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 18)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle.shape  # The kaggle data seems good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1623, 1143,  660, 1596,  846,  868,  145,  500,  652,  214, 1229,\n",
       "       1588, 1435,  115,  309,   70, 1068,   42,   47,  100,  165,  479,\n",
       "        977, 1292,  764,   31,   25, 1438, 1216,  712,  368, 1180, 1514,\n",
       "       1172, 1139,  835,  790,  893,  579, 1517, 1749,  402, 1570, 1049,\n",
       "        925, 1140,  138,  671,  541,  889,  845, 1217, 1148,  197, 1087,\n",
       "        612,  757, 1508,  720, 1685,  665,  738,  994, 1235,  576,  373,\n",
       "        851, 1323, 1648,  810,  569,   89, 1552,  352,  472, 1647,  740,\n",
       "        844, 1116,  921,  330,  658,  185, 1313,  849,   18,  662,  331,\n",
       "        279,  124,  182,  811,  396,  708,  523,   36,  956,  785,  528,\n",
       "        311,  876, 1198,  721, 1400,   72,  486, 1056,  906, 1048,  152,\n",
       "        228, 1371,   37, 1754, 1769,  186, 1332,  928, 1446,  532,  980,\n",
       "         91, 1224,  506, 1357, 1374, 1078,  983,  759, 1650,  635,  816,\n",
       "       1488, 1600, 1268, 1285, 1424,  156, 1582,  886, 1105,  258, 1748,\n",
       "       1007, 1617,  916, 1271, 1124,  631,  345,  617,  245, 1767,  584,\n",
       "       1525,  972,  457, 1561,  475, 1367,  420, 1069, 1739,  376,  975,\n",
       "        531, 1658,  367,  678, 1242,  262,  670, 1064, 1350,  198,  137,\n",
       "        866, 1300,  774, 1159, 1125, 1361,  507,  762,  277,  389, 1346,\n",
       "        874,   48, 1675,  647, 1720, 1590, 1152, 1226,  862, 1562, 1336,\n",
       "       1171])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essayid = kaggle.iloc[:,0].values  # extract essayid, because I can't use it to apply the model I built earlier\n",
    "essayid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_test = kaggle.iloc[:, 1:].values  # remove essayid column from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_test = sc.transform(k_test)  # Standardise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 5, 4, 4, 3, 3, 4, 3, 3,\n",
       "       2, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4,\n",
       "       4, 4, 3, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4,\n",
       "       2, 3, 3, 3, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 4, 4,\n",
       "       4, 3, 4, 4, 4, 3, 3, 4, 2, 3, 4, 4, 4, 3, 3, 4, 3, 4, 4, 3, 4, 4,\n",
       "       2, 3, 4, 4, 3, 4, 2, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 4, 3,\n",
       "       4, 3, 3, 3, 4, 4, 4, 3, 2, 3, 4, 4, 3, 3, 2, 4, 3, 4, 4, 3, 4, 4,\n",
       "       4, 3, 3, 4, 4, 2, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3,\n",
       "       3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_pred = clf.predict(k_test)  # Predict by applying the model that I found earlier\n",
    "kaggle_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1623, 1143,  660, 1596,  846,  868,  145,  500,  652,  214, 1229,\n",
       "        1588, 1435,  115,  309,   70, 1068,   42,   47,  100,  165,  479,\n",
       "         977, 1292,  764,   31,   25, 1438, 1216,  712,  368, 1180, 1514,\n",
       "        1172, 1139,  835,  790,  893,  579, 1517, 1749,  402, 1570, 1049,\n",
       "         925, 1140,  138,  671,  541,  889,  845, 1217, 1148,  197, 1087,\n",
       "         612,  757, 1508,  720, 1685,  665,  738,  994, 1235,  576,  373,\n",
       "         851, 1323, 1648,  810,  569,   89, 1552,  352,  472, 1647,  740,\n",
       "         844, 1116,  921,  330,  658,  185, 1313,  849,   18,  662,  331,\n",
       "         279,  124,  182,  811,  396,  708,  523,   36,  956,  785,  528,\n",
       "         311,  876, 1198,  721, 1400,   72,  486, 1056,  906, 1048,  152,\n",
       "         228, 1371,   37, 1754, 1769,  186, 1332,  928, 1446,  532,  980,\n",
       "          91, 1224,  506, 1357, 1374, 1078,  983,  759, 1650,  635,  816,\n",
       "        1488, 1600, 1268, 1285, 1424,  156, 1582,  886, 1105,  258, 1748,\n",
       "        1007, 1617,  916, 1271, 1124,  631,  345,  617,  245, 1767,  584,\n",
       "        1525,  972,  457, 1561,  475, 1367,  420, 1069, 1739,  376,  975,\n",
       "         531, 1658,  367,  678, 1242,  262,  670, 1064, 1350,  198,  137,\n",
       "         866, 1300,  774, 1159, 1125, 1361,  507,  762,  277,  389, 1346,\n",
       "         874,   48, 1675,  647, 1720, 1590, 1152, 1226,  862, 1562, 1336,\n",
       "        1171],\n",
       "       [   4,    3,    3,    4,    4,    4,    3,    3,    3,    3,    3,\n",
       "           4,    4,    3,    4,    4,    4,    4,    4,    3,    3,    3,\n",
       "           4,    4,    4,    4,    4,    4,    4,    3,    3,    4,    3,\n",
       "           3,    4,    3,    5,    4,    4,    3,    3,    4,    3,    3,\n",
       "           2,    3,    3,    4,    4,    3,    3,    4,    4,    4,    3,\n",
       "           4,    4,    4,    4,    4,    3,    4,    3,    4,    3,    4,\n",
       "           4,    4,    3,    4,    3,    4,    4,    3,    3,    3,    3,\n",
       "           4,    4,    4,    4,    4,    4,    3,    4,    3,    4,    4,\n",
       "           2,    3,    3,    3,    4,    3,    4,    4,    4,    4,    4,\n",
       "           3,    3,    4,    3,    3,    3,    4,    3,    3,    4,    4,\n",
       "           4,    3,    4,    4,    4,    3,    3,    4,    2,    3,    4,\n",
       "           4,    4,    3,    3,    4,    3,    4,    4,    3,    4,    4,\n",
       "           2,    3,    4,    4,    3,    4,    2,    4,    4,    4,    4,\n",
       "           3,    3,    4,    4,    3,    4,    4,    4,    3,    4,    3,\n",
       "           4,    3,    3,    3,    4,    4,    4,    3,    2,    3,    4,\n",
       "           4,    3,    3,    2,    4,    3,    4,    4,    3,    4,    4,\n",
       "           4,    3,    3,    4,    4,    2,    4,    4,    4,    4,    3,\n",
       "           3,    3,    3,    3,    4,    3,    4,    3,    4,    4,    3,\n",
       "           3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_joined = np.vstack((essayid, kaggle_pred))    # merge 2 arrays together, but vstack. not concatenate.\n",
    "array_joined  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = pd.DataFrame(array_joined)     # convert to dataframe\n",
    "final_submission = final_submission.transpose()   # flip the row and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essayid  score\n",
       "0     1623      4\n",
       "1     1143      3\n",
       "2      660      3\n",
       "3     1596      4\n",
       "4      846      4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission.columns = ['essayid', 'score']\n",
    "final_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('32134541-YoKogure-v1.csv', index=False)  # Exported to csv file\n",
    "# Refered to: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission complete. QWK of 0.62886 on the first try, I will try a little more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the end of my assignment 2. Before I tackle this assignment, I thought it was really hard because it was scary looking in the specification sheet. However, after looking through lectures, tutorial and actually trying these problems, I found out that I don't really have to be scared of this topic. It was more interesting and fun than I thought. \n",
    "<br>\n",
    "\n",
    "It was mindblowing that Python I use everyday contained the machine learning library, and could deal with this much data and analyze in very intuitive way. This gave me inspirations, and I look forward to upcoming R language. I had a fun time. Thank you for reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 32134541 Yo Kogure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saumya Awasthi. (2020). SEVEN MOST POPULAR SVM KERNELS. Retrieved from https://dataaspirant.com/svm-kernels/ <br>\n",
    "- Dhiraj K. (2019). Top 4 advantages and disadvantages of Support Vector Machine or SVM. Retrieved from https://dhirajkumarblog.medium.com/top-4-advantages-and-disadvantages-of-support-vector-machine-or-svm-a3c06a2b107 <br>\n",
    "- Tom Sharp. (2020). An Introduction to Support Vector Regression (SVR). Retrieved from https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2 <br>\n",
    "- Kernel Functions-Introduction to SVM Kernel & Examples. Retrieved from https://data-flair.training/blogs/svm-kernel-functions/ <br>\n",
    "- Kernel_method. Retrieved from https://en.wikipedia.org/wiki/Kernel_method <br>\n",
    "- 1.4. Support Vector Machines. Retrieved from https://scikit-learn.org/stable/modules/svm.html <br>\n",
    "- sklearn.metrics.cohen_kappa_score. Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html <br>\n",
    "- pandas.DataFrame.to_csv. Retrieved from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
